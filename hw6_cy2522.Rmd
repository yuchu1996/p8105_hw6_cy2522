---
title: "HW6"
author: "Chu YU"
date: "2018/11/22"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(modelr)
library(leaps)

```

## Problem 1

(1)The Washington Post has gathered data on homicides in 50 large U.S. cities and we will use it as our dataset.
```{r}
## import the data
homicide_df = read.csv("./data/homicide-data.csv") %>%
  mutate(city_state = str_c(city,",", state),
         solving_status = 
           ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), 0, 1)) %>%
  filter(!(city_state %in% c("Dallas,TX","Phoenix,AZ","Kansas City,MO","Tulsa,AL"))) %>%
  mutate(victim_race = ifelse(victim_race == "White", "white", "non-white"),
         victim_race = factor(victim_race, levels = c("white", "non-white")),
         victim_age = as.numeric(victim_age))

str(homicide_df)
  
```


**intoduction of the dataset :**

The Washington Post has identified the places in dozens of American cities where murder is common but arrests are rare. These pockets of impunity were identified by obtaining and analyzing up to a decade of homicide arrest data from 50 of the nationâ€™s largest cities. The analysis of 52,000 criminal homicides goes beyond what is known nationally about the unsolved cases, revealing block by block where police fail to catch killers.

**The data frame:**

The dataset "homicide_df" has 14 variables and 48507 observations, including the date and location of the crimes, the victims' personal information, and the deposition results.

After importing the dataset of homicides, I firstly used `mutate` to add a new variable representing the city and state of the data and another one to represent the status of the crimes. Then I deleted some useless rows and changed the variables "victim_race", "victim_age" into factors.



(2)linear regression model For the city of Baltimore, MD
```{r}
homicide_fit_logistic = homicide_df %>%
  filter(city_state == "Baltimore,MD") %>% 
  glm(solving_status ~ victim_age + victim_race + victim_sex, 
      family = binomial(), data = .)
 
homi_or = homicide_fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  select(term, OR)

homi_confidt = homicide_fit_logistic %>% 
  broom::confint_tidy() %>% 
    mutate(conf.low = exp(conf.low),
           conf.high = exp(conf.high))

cbind(homi_or, homi_confidt) %>% 
  knitr::kable(digits = 3)
```

As is required,For the city of Baltimore, MD, I used the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. 

By using `glm`, `broom::tidy`, `broom::confint_tidy`, I got a table about the odds ratios and confident intervals of the victims' age, race and sex in Baltimore, MD. 

   (3)For each of the cities
```{r}
or_confidt = function(homicide_data){
    fit_logistic = glm(solving_status ~ victim_age + victim_sex + victim_race, data = homicide_data, family = binomial())
    
    oddsr = fit_logistic %>% 
    broom::tidy() %>% 
    mutate(or = exp(estimate)) %>%
    select(term, or)
    
    confidt = fit_logistic %>% 
    broom::confint_tidy() %>% 
    mutate(conf.low = exp(conf.low),
           conf.high = exp(conf.high))
    
    output = cbind(oddsr, confidt) %>% 
      filter(term == "victim_racenon-white") %>% select(-term)
    
    output}



homicide_cities = homicide_df %>%
  group_by(city_state) %>%
  nest() %>%
  mutate(estimation = map(data, or_confidt)) %>%
  select(city_state, estimation) %>% unnest() 
  
homicide_cities %>% knitr::kable(digits = 3)
```

To do the plot regarding with each city in a tidy pipeline, I built a function to extract the OR and CIs of every city.And I got a table as above. There are 4 variables and 47 obs in the table, showing the adjusted ORS and CIs of each city.


  (4)Create a plot that shows the estimated ORs and CIs for each city. 
```{r}
homicide_cities %>%
  ggplot(aes(y = or,x = reorder(city_state, or))) + 
  geom_point() +
  geom_errorbar(aes(x = city_state, ymin = conf.low, ymax = conf.high)) +
  labs(
    title = "scatterplot of ORs and CIs for each city",
    x = "city and state name",
    y = "Solving status "
  ) +
   theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

From the scatterplot above, we can find that only "Tampa, FL", "Durham, NC" and "Birmingham, AL" has OR larger than 1.0, indicating that for most of the cities, the crimes whose victims are non-white may be more likely to be unsolved.
Then we can see that Durham, NC gets the largest CI among all the cities, meaning the adjusted OR of solving status of this city is not stable.


## problem 2
```{r}
## import the data and clean it
birthweight = read.csv("./data/birthweight.csv") %>%
  mutate(babysex = as.factor(babysex), 
         frace = as.factor(frace), 
         malform = as.factor(malform),
         mrace = as.factor(mrace))

## check for the missing values
sum(is.na(birthweight))

```

After importing and cleaning the dataset, I got a dataset with 20 variables and 4342 obs.

As is required, I transferred the variables "babysex", "frace", "malform", "mrace" into factors. Then I checked the missing values of the data and found there is no missing values in it.

  (1)Propose a regression model for birthweight.
```{r}
##using stepwise to  
mult.fit = lm(bwt ~ ., data = birthweight)
step(mult.fit, direction = 'backward')
```

By using the "stepwise regression", I finally choose "bhead", "blength", "mrace", "parity" as the predictors, which I think have higher association with babyweight.

```{r}
bwt_fit = lm(bwt ~ bhead + blength + mrace + parity, data = birthweight)

summary(bwt_fit)
```

We then get the bwt_fit model: bwt ~ bhead + blength + mrace + parity. And from the further analysis of it, we can get adjustedd R-squared 0.6962, according to which I think I choose the right variables.


```{r}
bwt_pre_res = birthweight %>% 
    add_predictions(model = bwt_fit, var = "pred") %>% 
    add_residuals(model = bwt_fit, var = "resid")

bwt_pre_res %>% 
    ggplot(aes(x = pred, y = resid)) +
    geom_point(alpha = 0.4) +
    geom_smooth() +
    labs(
        y = "Residuals",
        x = "Predictions",
        title = "plot of model residuals against fitted values"
    )
```

By using "add_predictions" and "add_residuals" I got a scatterplot with smooth line as above.

We can see from the plot that when predictions are approximately from 2000 to 4000, the residuals are near zero, indicating that further the predictions are away from this interval, less likely are the residuals normal. So this model is most valid in this interval.


  (3)Compare your model to two others
```{r}
## using cv to compare three different lm models
cv_df =
  crossv_mc(birthweight, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) %>%
  mutate(bwt_myfit1 = map(train, ~lm(bwt ~ bhead + blength + mrace + parity, data = .x)),
         bwt_fit_compare1 = map(train, ~lm(bwt ~ gaweeks + blength , data = .x)),
         bwt_fit_compare2 = map(train, ~lm(bwt ~ bhead + blength + babysex +babysex * blength + babysex * bhead + bhead * babysex + bhead * babysex * blength, data = .x))) %>% 
  mutate(rmse_myfit  = map2_dbl(bwt_myfit1, test, ~rmse(model = .x, data = .y)),
         rmse_compare1 = map2_dbl(bwt_fit_compare1, test, ~rmse(model = .x, data = .y)),
         rmse_compare2 = map2_dbl(bwt_fit_compare2, test, ~rmse(model = .x, data = .y)))
```

I built a CV data set to compare the three different linear regression models. 

(4)a plot for comoparing
```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```
 
 I got three different models as below:
 
  - bwt_myfit1 : bwt ~ bhead + blength + mrace + parity;
  
  - bwt_fit_compare1 : bwt ~ gaweeks + blength;
  
  - bwt_fit_compare2 : bwt ~ bhead + blength + babysex +babysex * blength + babysex * bhead + bhead * babysex + bhead * babysex * blength
  
  From the violin plot, we can see that the second model has the significantly highest RMSE among three models, and my model has the lowest RMSE. So I can conclude that my model:  bwt ~ bhead + blength + mrace + parity is better than the other two models and the second model is the worst.